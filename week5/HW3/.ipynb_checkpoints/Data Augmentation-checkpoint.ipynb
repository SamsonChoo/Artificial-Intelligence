{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms,  models\n",
    "from getimagenetclasses import parsesynsetwords, parseclasslabel, get_classes\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"imagenet2500/imagespart/\"\n",
    "lbl_dir = \"ILSVRC2012_bbox_val_v3/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydata(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, lbl_dir, transform=None):\n",
    "\n",
    "        self.img_name_list = os.listdir(img_dir)     # list of names of images\n",
    "        self.xml_list = os.listdir(lbl_dir)[:len(self.img_name_list)]     # list of the first 2500 xml\n",
    "        self.lbl_list = []     # list of the labels\n",
    "        self.transform = transform\n",
    "\n",
    "        filen = 'synset_words.txt'\n",
    "        indicestosynsets, synsetstoindices, synsetstoclassdescr = parsesynsetwords(filen)\n",
    "\n",
    "        for xml in self.xml_list:\n",
    "            label = parseclasslabel(lbl_dir + xml, synsetstoindices)[0] # get the label/index\n",
    "            self.lbl_list.append(label)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_name_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.lbl_list[idx]        \n",
    "        image = Image.open(img_dir + self.img_name_list[idx]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            return self.transform(image),label     # does transformation if exists\n",
    "    \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-c3a03be2a680>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-c3a03be2a680>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    self.transform = transforms.Compose([\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "##Transforming using pytorch functions\n",
    "        self.transform = transforms.Compose([\n",
    "                    transforms.Resize(224),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "        if normalize == False:\n",
    "            self.transform = transforms.Compose([\n",
    "                        transforms.Resize(224),\n",
    "                        transforms.CenterCrop(224),\n",
    "                        transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_by_hand(k,bilinear=True):\n",
    "    img = Mydata(img_dir,lbl_dir)[k][0]\n",
    "    ratio = min(img.width/224, img.height/224)     # get img ratio according to shorter side \n",
    "    if (bilinear):\n",
    "        img = img.resize((int(img.width/ratio), int(img.height/ratio)), Image.BILINEAR)\n",
    "    else:\n",
    "        img = img.resize((int(img.width/ratio), int(img.height/ratio)))\n",
    "\n",
    "    img = np.array(img)\n",
    "    img = img/255     # normalization\n",
    "    img = img.transpose(2,0,1)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    # pytorch standard for imagenet normalization\n",
    "    m = np.array([0.485, 0.456, 0.406])\n",
    "    s = np.array([0.229, 0.224, 0.225])\n",
    "    img = (img-m[np.newaxis, :, np.newaxis, np.newaxis])/(s[np.newaxis, :, np.newaxis, np.newaxis])\n",
    "    #print(img.shape)\n",
    "    height,width = img.shape[2:4]\n",
    "    h_bot = int(np.ceil((height-224)/2))\n",
    "    h_top = h_bot + 224\n",
    "    w_left = int(np.ceil((width-224)/2))\n",
    "    w_right = w_left + 224\n",
    "    img = img[:,:,h_bot:h_top,w_left:w_right]\n",
    "    #print(img.shape)\n",
    "    return torch.tensor(img).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(k=0):\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(224),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    data1 = Mydata(img_dir,lbl_dir,transform)[k][0]\n",
    "    data2 = rescale_by_hand(k)\n",
    "    print(type(data1))\n",
    "    print(type(data2))\n",
    "    d = data2 - data1\n",
    "    \n",
    "    print(\"Tensor diff (with bilinear resize): \", d)\n",
    "    print(\"Squared sum diff (with bilinear resize): \", torch.sum(d**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "Tensor diff:  tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-1.1921e-07,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-1.1921e-07,  0.0000e+00, -1.1921e-07,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-1.1921e-07,  0.0000e+00, -1.1921e-07,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-1.1921e-07, -2.3842e-07, -2.3842e-07,  ..., -1.1921e-07,\n",
      "           -1.1921e-07, -1.1921e-07],\n",
      "          [-1.1921e-07, -1.1921e-07, -1.1921e-07,  ..., -1.1921e-07,\n",
      "           -1.1921e-07, -1.1921e-07],\n",
      "          [-1.1921e-07, -1.1921e-07, -1.1921e-07,  ..., -1.1921e-07,\n",
      "           -1.1921e-07, -1.1921e-07],\n",
      "          ...,\n",
      "          [-1.1921e-07, -2.3842e-07, -1.1921e-07,  ..., -2.3842e-07,\n",
      "           -2.3842e-07, -2.3842e-07],\n",
      "          [-1.1921e-07, -2.3842e-07, -1.1921e-07,  ..., -2.3842e-07,\n",
      "           -2.3842e-07, -1.1921e-07],\n",
      "          [-2.3842e-07, -1.1921e-07, -1.1921e-07,  ..., -2.3842e-07,\n",
      "           -1.1921e-07, -2.3842e-07]]]])\n",
      "Squared sum diff:  tensor(1.6001e-09)\n"
     ]
    }
   ],
   "source": [
    "diff(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_no_bilinear(k=0):\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(224),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    data1 = Mydata(img_dir,lbl_dir,transform)[k][0]\n",
    "    data2 = rescale_by_hand(k,False)\n",
    "    print(type(data1))\n",
    "    print(type(data2))\n",
    "    d = data2 - data1\n",
    "    \n",
    "    print(\"Tensor diff(without bilinear resize): \", d)\n",
    "    print(\"Squared sum diff(without bilinear resize): \", torch.sum(d**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "Tensor diff:  tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -1.7125e-02,  0.0000e+00,  ...,  1.7125e-02,\n",
      "            0.0000e+00,  3.4249e-02],\n",
      "          [-1.7125e-02,  1.7125e-02,  1.7125e-02,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.1374e-02, -8.5624e-02, -1.5412e-01,  ..., -5.1374e-02,\n",
      "           -5.1374e-02,  8.5624e-02],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -5.1374e-02,\n",
      "            1.1987e-01, -1.5412e-01],\n",
      "          [ 0.0000e+00, -3.4250e-02, -1.7125e-02,  ..., -1.0275e-01,\n",
      "           -1.3700e-01,  6.8499e-02]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00, -3.5014e-02],\n",
      "          [-1.7507e-02,  0.0000e+00,  0.0000e+00,  ...,  1.7507e-02,\n",
      "            0.0000e+00,  1.7507e-02],\n",
      "          [-1.7507e-02,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.2521e-02, -8.7535e-02, -1.7507e-01,  ..., -5.2521e-02,\n",
      "           -3.5014e-02,  7.0028e-02],\n",
      "          [ 1.7507e-02,  0.0000e+00, -1.1921e-07,  ..., -3.5014e-02,\n",
      "            1.2255e-01, -1.5756e-01],\n",
      "          [ 3.5014e-02, -3.5014e-02, -1.7507e-02,  ..., -1.2255e-01,\n",
      "           -1.4006e-01,  7.0028e-02]],\n",
      "\n",
      "         [[ 1.7429e-02, -2.3842e-07, -2.3842e-07,  ..., -1.1921e-07,\n",
      "           -1.1921e-07, -3.4859e-02],\n",
      "          [-1.1921e-07, -1.1921e-07, -1.1921e-07,  ...,  1.7429e-02,\n",
      "           -1.1921e-07,  1.7429e-02],\n",
      "          [-1.7429e-02,  1.7429e-02, -1.1921e-07,  ..., -1.1921e-07,\n",
      "           -1.1921e-07, -1.1921e-07],\n",
      "          ...,\n",
      "          [-5.2288e-02, -6.9717e-02, -1.3943e-01,  ..., -5.2288e-02,\n",
      "           -3.4859e-02,  6.9717e-02],\n",
      "          [ 1.7429e-02, -2.3842e-07, -1.7429e-02,  ..., -3.4859e-02,\n",
      "            1.2200e-01, -1.5686e-01],\n",
      "          [ 5.2287e-02, -3.4859e-02, -1.7429e-02,  ..., -1.0458e-01,\n",
      "           -1.3943e-01,  3.4858e-02]]]])\n",
      "Squared sum diff:  tensor(2846.6577)\n"
     ]
    }
   ],
   "source": [
    "diff_no_bilinear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_normalization(model,n=250):     # where n is the number of images evaluated\n",
    "    \n",
    "    transform1 = transforms.Compose([\n",
    "                    transforms.Resize(224),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    \n",
    "    # no normalization\n",
    "    transform2 = transforms.Compose([\n",
    "                    transforms.Resize(224),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor()])\n",
    "    \n",
    "    \n",
    "    data1 = Mydata(img_dir,lbl_dir,transform1)\n",
    "    data2 = Mydata(img_dir,lbl_dir,transform2)\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(n):\n",
    "        img,lbl = data1[i]\n",
    "        pred = model(torch.unsqueeze(img,0))\n",
    "        _, index = torch.max(pred,1)\n",
    "        if lbl == index[0].item():\n",
    "            correct+=1\n",
    "    norm_acc = correct/n\n",
    "    print(\"Accuracy of prediction on normalized data is \" + str(norm_acc))\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(n):\n",
    "        img,lbl = data2[i]\n",
    "        pred = model(torch.unsqueeze(img,0))\n",
    "        _, index = torch.max(pred,1)\n",
    "        if lbl == index[0].item():\n",
    "            correct+=1\n",
    "    unnorm_acc = correct/n\n",
    "    print(\"Accuracy of prediction on un-normalized data is \" + str(unnorm_acc))\n",
    "    \n",
    "    diff_acc = norm_acc - unnorm_acc\n",
    "    print(\"The difference in accuracy/loss over \" + str(n) + \" images is \" + str(diff_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    diff()\n",
    "    diff_no_bilinear()\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.eval()\n",
    "    diff_normalization(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction on normalized data is 0.688\n",
      "Accuracy of prediction on un-normalized data is 0.464\n",
      "The difference in accuracy/loss over 250 images is 0.22399999999999992\n"
     ]
    }
   ],
   "source": [
    "diff_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_crop(model,n=250):     # where n is the number of images\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize(280),\n",
    "            transforms.FiveCrop(224),\n",
    "            transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "            transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(crop) for crop in crops]))])\n",
    "\n",
    "    data = Mydata(img_dir,lbl_dir,transform)\n",
    "    \n",
    "    correct = 0\n",
    "    overall_score = 0\n",
    "    for i in range(n):     # iterage over each image\n",
    "        for j in range(5):     # iterate over each crop            \n",
    "            img,lbl = data[i][0][j], data[i][1]\n",
    "            pred = model(torch.unsqueeze(img,0))\n",
    "            score = torch.nn.functional.softmax(pred, dim=1)[0]\n",
    "            overall_score += score\n",
    "            \n",
    "        overall_score = overall_score/5\n",
    "        _, index = torch.max(overall_score,0)\n",
    "        \n",
    "        if lbl == index.item():\n",
    "            correct+=1\n",
    "            \n",
    "        norm_acc = correct/n\n",
    "    print(\"Accuracy of prediction is \" + str(norm_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction is 0.7\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "    model2 = modedls.mobilenet(pretrained=True)\n",
    "five_crop(model,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=50\n",
    "model2 = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(400),\n",
    "        transforms.FiveCrop(330),\n",
    "        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "        transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(crop) for crop in crops]))])\n",
    "\n",
    "data=Mydata(img_dir,lbl_dir,transform)\n",
    "\n",
    "correct = 0\n",
    "for i in range(n):\n",
    "    img,lbl = data1[i]\n",
    "    pred = model(torch.unsqueeze(img,0))\n",
    "    _, index = torch.max(pred,1)\n",
    "    if lbl == index[0].item():\n",
    "        correct+=1\n",
    "norm_acc = correct/n\n",
    "print(\"Accuracy of prediction on resnet is \" + str(norm_acc))\n",
    "\n",
    "correct = 0\n",
    "for i in range(n):\n",
    "    img,lbl = data1[i]\n",
    "    pred = model2(torch.unsqueeze(img,0))\n",
    "    _, index = torch.max(pred,1)\n",
    "    if lbl == index[0].item():\n",
    "        correct+=1\n",
    "norm_acc = correct/n\n",
    "print(\"Accuracy of prediction on mobilenet is \" + str(norm_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
